{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Pipeline').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 97237\n",
      "+-------+--------------------+\n",
      "|overall|          reviewText|\n",
      "+-------+--------------------+\n",
      "|    4.0|They look good an...|\n",
      "|    5.0|These stickers wo...|\n",
      "|    5.0|awesome! stays on...|\n",
      "|    5.0|Came just as desc...|\n",
      "|    5.0|Performs exactly ...|\n",
      "+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load reviews fro json file\n",
    "data_frame = spark.read.json('../data/reviews.json') \\\n",
    "                       .select('overall', 'reviewText') \\\n",
    "                       .sample(False, fraction=0.5)\n",
    "\n",
    "print('Number of reviews: %d' % data_frame.count())\n",
    "data_frame.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|overall|          reviewText|\n",
      "+-------+--------------------+\n",
      "|    3.0|they look good an...|\n",
      "|    4.0|these stickers wo...|\n",
      "|    4.0|awesome! stays on...|\n",
      "|    4.0|came just as desc...|\n",
      "|    4.0|performs exactly ...|\n",
      "+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "# TODO: convert lines to lower case using data_frame.rdd.map transformation\n",
    "\n",
    "data_frame_lower = spark.createDataFrame(\n",
    "         data_frame.rdd.map(lambda r: Row(\n",
    "                              overall=r.overall - 1,\n",
    "                              reviewText=r.reviewText.lower())))\n",
    "\n",
    "data_frame_lower.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(overall=0.0),\n",
       " Row(overall=1.0),\n",
       " Row(overall=4.0),\n",
       " Row(overall=3.0),\n",
       " Row(overall=2.0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: show distinct ratings\n",
    "data_frame_lower.select('overall').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "?LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# TODO: preprocess text\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='reviewText', outputCol='tokenized')\n",
    "\n",
    "with open('../data/stopwords.txt') as src:\n",
    "    stop_words_list = [word.strip() for word in src]\n",
    "    \n",
    "stop_words = StopWordsRemover(inputCol=tokenizer.getOutputCol(),\n",
    "                              outputCol='stop_words',\n",
    "                              stopWords=stop_words_list)\n",
    "\n",
    "ngram = NGram(n=2,\n",
    "              inputCol=stop_words.getOutputCol(), \n",
    "              outputCol='ngram')\n",
    "\n",
    "hashing = HashingTF(numFeatures=512,\n",
    "                    binary=True,\n",
    "                    inputCol=ngram.getOutputCol(),\n",
    "                    outputCol='hashing')\n",
    "\n",
    "logreg = LogisticRegression(featuresCol=hashing.getOutputCol(),\n",
    "                           labelCol='overall',\n",
    "                           predictionCol='prediction',\n",
    "                           family='multinomial')\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer,\n",
    "                     stop_words, \n",
    "                     ngram,\n",
    "                     hashing,\n",
    "                     logreg])\n",
    "\n",
    "model = pipeline.fit(data_frame_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: get pipeline prediction using transform()\n",
    "prediction = model.transform(data_frame_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <object repr() failed>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-2.1.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 76, in __del__\n",
      "    SparkContext._active_spark_context._gateway.detach(self._java_obj)\n",
      "AttributeError: 'HashingTF' object has no attribute '_java_obj'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 150    0    1    0   16]\n",
      " [   0  131    1    2   17]\n",
      " [   0    1  151   14  110]\n",
      " [   1    1   18  224  274]\n",
      " [   6    8   52   96 1269]]\n",
      "mse: 0.650412898152\n",
      "acc: 0.756979944947\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# TODO: estimate quality\n",
    "overall = prediction.select('overall').collect()\n",
    "predicted = prediction.select('prediction').collect()\n",
    "\n",
    "print('%s' % confusion_matrix(overall, predicted))\n",
    "print('mse: %s' % mean_squared_error(overall, predicted))\n",
    "print('acc: %s' % accuracy_score(overall, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# TODO: build param grid\n",
    "# TODO: find best param match using accuracy as a target metric\n",
    "?CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ParamGridBuilder()\\\n",
    "  .addGrid(hashing.numFeatures, [128, 1024])\\\n",
    "  .addGrid(logreg.regParam, [1e-3])\\\n",
    "  .build()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', \n",
    "                                              labelCol='overall',\n",
    "                                              metricName='accuracy')\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=params, evaluator=evaluator)\n",
    "\n",
    "crossval_model = cv.fit(data_frame_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Param(parent='LogisticRegression_402eb7f3ffe17dbade4d', name='regParam', doc='regularization parameter (>= 0).'): 0.001, Param(parent='HashingTF_4cc99369eadcf34c56fc', name='numFeatures', doc='number of features.'): 128} 0.5588109524920779\n",
      "{Param(parent='LogisticRegression_402eb7f3ffe17dbade4d', name='regParam', doc='regularization parameter (>= 0).'): 0.001, Param(parent='HashingTF_4cc99369eadcf34c56fc', name='numFeatures', doc='number of features.'): 1024} 0.5523188665849588\n"
     ]
    }
   ],
   "source": [
    "# output average metric for each param set\n",
    "for accuracy, params in zip(crossval_model.avgMetrics, cv.getEstimatorParamMaps()):\n",
    "    print(params, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
